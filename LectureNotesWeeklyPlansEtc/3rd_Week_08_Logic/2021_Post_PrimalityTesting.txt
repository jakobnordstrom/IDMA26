Primality testing and randomized algorithms

I just wanted to write down two quick comments about the primality testing we discussed the other lecture.

We saw an algorithm that tested whether m was prime by testing for all numbers n smaller than the square root of m whether m was divisible by n. This algorithm clearly takes time O(sqrt(m)) if we perform all arithmetic operations at unit cost. For algorithms dedicated specifically to crunching numbers, though (such as algorithms for division, multiplication, factoring, or primality testing), the size of the input should be considered to be the number of bits, which is log(m) (rounded up, and where all logs I take will always be base-2 unless specified otherwise). Measured in this way, the time complexity scales like sqrt(2^(log m)) = 2^{(log m)/2}, which is exponential.

I then mentioned that there are (much) more efficient randomized algorithms, but did not explain what a "randomized algorithm" is. Briefly, a randomized algorithm is an algorithm that we imagine to have access to a source of randomness (say, perfectly random coin flips, or uniformly and independently distributed random integers between 1 and 100). Because of the use of randomness, the algorithm could have some probability of error, but it should be very small (say, less than 1% or so). There are some problems for which the best algorithms known use randomness, and for which the currently best standard algorithms without randomness, called deterministic algorithms, are much slower. 

Most programming languages have a built-in random function, but this function only provides "fake randomness", or pseudo-randomness to use formal terminology. Whether true randomness actually exists in reality is a deep philosophical question, and this is not really the remit of theoretical computer science. What we can do in TCS is to assume that randomness does exist, and then see what algorithms can be constructed using this assumption. When doing so, another deep question that arises is whether access to true randomness would make it possible to solve more problems efficiently (i.e., in polynomial time) on a computer. This is a big open problem in theoretical computer science (the question of "P vs. BPP", if you want to search for more info on the internet), but most researchers believe that randomness actually does not buy you anything much. The reason for this, in turn, is that it is believed that one can construct pseudo-random generators that deterministically generate output that is clearly not random in the strict, mathematical sense, but looks "similar enough" to the real stuff that no efficient algorithm can tell it apart from true randomness. If it is possible to construct such pseudo-random generators, then you can take a randomized algorithm and feed it this fake randomness, and it will still run as fast as it would for true randomness (because if it didn't, then this could be used to tell pseudo-randomness apart from true randomness). This is an area of very active, and deep and fascinating, research, with a lot of beautiful math.

But, just to be clear, this discussion is well beyond the scope of this course. However, just in case some of you were wondering about these strange words that I was throwing around during the lecture, I thought I could try to elaborate briefly for those of you who might be interested.
