After the Monday lecture week 2

Hi all,

Since time was a little bit tight at the end, let me just recap a few important points in the lecture in reverse chronological order.

Towards the end of the lectures we did some proofs, or what seemed to maybe be proofs, of the identities \(\sum_{i=1}^{n}i = \frac{n(n+1)}{2}\) and \(\sum_{i=1}^{n} c^i = \frac{c^{n+1} - c}{c - 1}\).

What we did was to show that
(a) The formulas are correct for small values of \(n\).
(b) If the formulas are correct for \(n\), then they are also correct for  \(n + 1\).

The question left hanging at the end of the lecture is whether these are valid, formal proofs and/or how we make sure that arguments like this are mathematically valid. This is related to mathematical induction, which we will talk more about on Wednesday and next week.

We also talked about the difference between big-oh and little-oh. The intuition I tried to convey (which is NOT the formal definition, but is close to the formal definition) is as follows:

Saying that \(f(n)\) is \(O(g(n))\) means that  \(f(n)\) does not grow asymptotically faster than \(g(n)\), or, expressed differently, that  \(\frac{f(n)}{g(n)} \leq K\) for some constant \(K\) as \(n\) gets large enough.

Saying that \(f(n)\) is \(o(g(n))\) is the much stronger statement that \(f(n)\) is negligible compared to \(g(n)\) as \(n\) gets large enough, or, expressed differently, that \(\frac{f(n)}{g(n)} \to 0\) as \(n\) goes to infinity.

It is absolutely crucial that you make sure to get comfortable with big-oh notation and what it means. A good intuition is to think that big-oh means "remove all lower-order terms and also the coefficient in front of the higest-order term." As an example, if \(f(n) = 5 n^3 + 7 n^2 - 11 n - 13\), we shave off everything except  \(5 n^3\) and then also remove the coefficient \(5\) to get that \(f(n)\) is \(O(n^3)\). But in order to make this mathematically precise we need a formal definition, and this is what you find in the course literature (where CLRS is the recommended source for all things algorithmic, although asymptotic notation is covered in both textbooks, I believe).

It is also absolutely crucial that you digest all the notions and all the notation introduced today. This is our language going forward, and we will all need to be fluent in this language to have a good conversation.

Finally, I mentioned as an extra exercise that you can prove that \(\sum_{i=1}^{n} \frac{1}{i}\) is \(\Theta(\log n)\) by comparing \(\sum_{i=1}^{n} \frac{1}{i}\) and \(\int_{x=1}^{n} \frac{1}{x} d x \) and using that \(\frac{d \ln x}{d x} = \frac{1}{x}\).

See you again on Wednesday!

